Getting started
===============

Writing and running a job
-------------------------

To create your own map reduce job, subclass :py:class:`~mrjob.job.MRJob`, create a
series of mappers and reducers, and override :py:meth:`~mrjob.job.MRJob.steps`. For example, a word counter::

    from mrjob.job import MRJob

    class MRWordCounter(MRJob):
        def get_words(self, key, line):
            for word in line.split():
                yield word, 1

        def sum_words(self, word, occurrences):
            yield word, sum(occurrences)

        def steps(self):
            return [self.mr(self.get_words, self.sum_words),]

    if __name__ == '__main__':
        MRWordCounter.run()

**The two lines at the bottom are mandatory.** This is what allows your class
to be run by Hadoop streaming.

This will take in a file with lines of whitespace separated words, and
output a file with tab-separated lines like: ``"stars"\t5``.

For one-step jobs, you can also just redefine :py:meth:`~mrjob.job.MRJob.mapper` and :py:meth:`~mrjob.job.MRJob.reducer`::

    from mrjob.job import MRJob

    class MRWordCounter(MRJob):
        def mapper(self, key, line):
            for word in line.split():
                yield word, 1

        def reducer(self, word, occurrences):
            yield word, sum(occurrences)

    if __name__ == '__main__':
        MRWordCounter.run()

Running locally
^^^^^^^^^^^^^^^

To test the job locally, just run::

   python your_mr_job_sub_class.py < log_file_or_whatever > output

The script will automatically invoke itself to run the various steps,
using :py:class:`~mrjob.local.LocalMRJobRunner`.

You can also run individual steps:

.. code-block:: sh

    # test 1st step mapper:
    python your_mr_job_sub_class.py --mapper
    # test 2nd step reducer (--step-num=1 because step numbers are 0-indexed):
    python your_mr_job_sub_class.py --reducer --step-num=1

By default, we read from stdin, but you can also specify one or more
input files. It automatically decompresses .gz and .bz2 files::

    python your_mr_job_sub_class.py log_01.gz log_02.bz2 log_03

See :py:mod:`mrjob.examples` for more examples.

Running on EMR
^^^^^^^^^^^^^^

* Set up your Amazon Account (see :ref:`amazon-setup`)
* Set :envvar:`AWS_ACCESS_KEY_ID` and :envvar:`AWS_SECRET_ACCESS_KEY`
* Run your job with ``-r emr``::

    python your_mr_job_sub_class.py -r emr < input > output

Running on your own Hadoop cluster
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* Set up a hadoop cluster (see http://hadoop.apache.org/common/docs/current/)
* If running Python 2.5 on your cluster, install the :py:mod:`simplejson` module on all nodes. (Recommended but not required for Python 2.6+)
* Make sure :envvar:`HADOOP_HOME` is set
* Run your job with ``-r hadoop``::

    python your_mr_job_sub_class.py -r hadoop < input > output

Running from another script
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Use :py:meth:`~mrjob.job.MRJob.make_runner` to run an
:py:class:`~mrjob.job.MRJob` from another Python script::

    from __future__ import with_statement # only needed on Python 2.5

    mr_job = MRWordCounter(args=['-r', 'emr'])
    with mr_job.make_runner() as runner:
        runner.run()
        for line in runner.stream_output():
            key, value = mr_job.parse_output_line(line)
            ... # do something with the parsed output

Common configuration tasks
--------------------------

Putting your source tree in the :envvar:`PYTHONPATH`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If your job spans multiple files, you can create a tarball of your source tree
and use ``python_archives`` to have it decompressed and added to the
:envvar:`PYTHONPATH`::

    runners:
      emr:  # this will work for any runner
        python_archives:
        - my-src-tree.tar.gz

It will probably be convenient to have the tarball generated by your build
process.

Increasing the task timeout
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. warning::

    Some EMR AMIs appear to not support setting parameters like
    timeout with ``jobconf`` at run time. Instead, you must use
    :ref:`bootstrap-time-configuration`.

If your mappers or reducers take a long time to process a single step, you may
want to increase the amount of time Hadoop lets them run before failing them
as timeouts. You can do this with ``jobconf`` and the version-appropriate
Hadoop environment variable. For example, this configuration will set the
timeout to one hour::

    runners:
        hadoop: # this will work for both hadoop and emr
            jobconf:
                # Hadoop 0.18
                mapred.task.timeout: 3600000
                # Hadoop 0.21+
                mapreduce.task.timeout: 3600000

mrjob will convert your ``jobconf`` options between Hadoop versions if
necessary. In this example, either ``jobconf`` line could be removed and the
timeout would still be changed when using either version of Hadoop.

Writing compressed output
^^^^^^^^^^^^^^^^^^^^^^^^^

To save space, you can have Hadoop automatically save your job's output as
compressed files. This can be done using the same method as changing the task
timeout, with ``jobconf`` and the appropriate environment variables. This
example uses the Hadoop 0.21+ version::

    runners:
        hadoop: # this will work for both hadoop and emr
            jobconf:
               # "true" must be a string argument, not a boolean! (#323)
               mapreduce.output.compress: "true"
               mapreduce.output.compression.codec: org.apache.hadoop.io.compress.GzipCodec

Common EMR configuration tasks
------------------------------

Custom Python packages
^^^^^^^^^^^^^^^^^^^^^^

There are a couple of ways to install Python packages that are not in the
standard library. If there is a Debian package, you can add a call to
``apt-get`` as a ``bootstrap_cmd``::

    runners:
      emr:
        bootstrap_cmds:
        - sudo apt-get install -y python-simplejson

If there is no Debian package or you prefer to use your own tarballs for some
other reason, you can specify tarballs in ``bootstrap_python_packages``, which
supports glob syntax::

    runners:
      emr:
        bootstrap_python_packages:
        - $MY_SOURCE_TREE/emr_packages/*.tar.gz

.. _bootstrap-time-configuration:

Bootstrap-time configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some Hadoop options, such as the maximum number of running map tasks per node,
must be set at bootstrap time and will not work with `--jobconf`. You must use
Amazon's `configure-hadoop` script for this. For example, this limits the
number of mappers and reducers to one per node::

    --bootstrap-action="s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    -m mapred.tasktracker.map.tasks.maximum=1 \
    -m mapred.tasktracker.reduce.tasks.maximum=1"

Setting up Ganglia
^^^^^^^^^^^^^^^^^^

`Ganglia <http://www.ganglia.info>`_` is a scalable distributed monitoring
system for high-performance computing systems. You can enable it for your
EMR cluster with Amazon's `install-ganglia`_ bootstrap action::

    --bootstrap-action="s3://elasticmapreduce/bootstrap-actions/install-ganglia

.. _install-ganglia: http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/index.html?init_Ganglia.html

Enabling Python core dumps
^^^^^^^^^^^^^^^^^^^^^^^^^^

Particularly bad errors may leave no traceback in the logs. To enable core
dumps on your EMR instances, put this script in ``core_dump_bootstrap.sh``::

    #!/bin/sh

    chk_root () {
        if [ ! $( id -u ) -eq 0 ]; then
            exec sudo sh ${0}
            exit ${?}
        fi
    }

    chk_root

    mkdir /tmp/cores
    chmod -R 1777 /tmp/cores
    echo "\n* soft core unlimited" >> /etc/security/limits.conf
    echo "ulimit -c unlimited" >> /etc/profile
    echo "/tmp/cores/core.%e.%p.%h.%t" > /proc/sys/kernel/core_pattern

Use the script as a bootstrap action in your job::

    --bootstrap-action=core_dump_setup.sh

You'll probably want to use a version of Python with debugging symbols, so
install it and use it as ``python_bin``::

    --bootstrap-cmd="sudo apt-get install -y python2.6-dbg" \
    --python-bin=python2.6-dbg

Run your job in a persistent job flow. When it fails, you can SSH to your nodes
to inspect the core dump files::

    you@local: emr --ssh j-MYJOBFLOWID

    hadoop@ip-10-160-75-214:~$ gdb `which python` /tmp/cores/core.python.blah

If you have multiple nodes, you may have to :command:`scp` your identity file
to the master node and use it to SSH to the slave nodes, where the core dumps
are located::

    hadoop@ip-10-160-75-214:~$ hadoop dfsadmin -report | grep ^Name
    Name: 10.166.50.85:9200
    Name: 10.177.63.114:9200

    hadoop@ip-10-160-75-214:~$ ssh -i uploaded_key.pem 10.166.50.85

    hadoop@ip-10-166-50-85:~$ gdb `which python2.6-dbg` /tmp/cores/core.python.blah
